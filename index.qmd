---
title: "Logistic Regression Analysis Example: Optimizing Expected Revenue"
date: "2026-01-13"
description: "A simulated study on how to build a logistic regression model and utilize it to optimize a performance metric"
categories:
  - "Machine Learning"
  - "Logistic Regression"
  - "Optimization"
engine: knitr
freeze: auto
toc: true
toc-depth: 2
toc-location: left
bibliography: bibliography.bib
execute:
  results: hold
---

Last Update: `r format(Sys.Date(),"%B %d, %Y")`

All the packages used in this project are below.

```{r libraries}
#| warning: false
#| message: false
library(knitr)
library(kableExtra)
library(arm)
library(gtsummary)
library(dplyr)
library(ggplot2)
library(future.apply)
library(scales)
```

# Introduction

This will be the one of the first in a series of example analyses I plan to post online to showcase my statistical technical knowledge as well as document some interesting problems I have come across in my experience. I specifically want to focus on what to do over the course of a logistic regression and why I make certain decisions over the course of the analysis to solve the question at hand.

This project is organized into the following parts:

1.  A basic introduction of the logistic regression model and its comparison to other machine learning models

2.  A simulated example detailing the workflow to build a logistic regression model [jump to section](#prob)

3.  A method to leverage the logistic regression model to optimize some target goal [jump to section](#sec-optimize)

In this project, the data is simulated to mirror the format of the original application problem and to ease the model building process to really focus on the optimization methodology. Though the model building will be as perfect as perfect can be, I will give guidance on options to consider when conditions are less ideal.

The material here details the process for conducting a logistic regression analysis and summarizes about 1-2 college semesters of statistics courses in one document. Readers who are already familiar with logistic regression models may want to skip to the section that details the analysis question and then skip to the section on how to optimize for a target goal.

# Logistic Regression

Logistic regression is one of the fundamental tools of machine learning and statistics and is actually one of the more powerful tools when one has enough time to go that extra mile and fine tune the model to the data. First I'll just state the mathematical formula for logistic regression beginning with the data model:

$Y_i|\pi_i \sim Bernoulli(\pi_i)$

where $i$ refers to the data sample index (think of a row in your data table), the outcome variable $Y_i$ is whether "success" is observed for sample $i$, and $\pi_i$ is the probability of success for the $i$-th data point. In typical applications, $Y_i$ is coded such that 0 means "failure" and 1 means "success". Now we can move on to the regression part of the model between the log-odds and the covariates of interest:

$\log(\frac{\pi_i}{1-\pi_i}) = \pmb x_i \pmb \beta$

where $\pmb x_i$ is a vector of dependent variables or covariates that you think explain the probability of success and $\pmb \beta$ is the vector of weights that indicate how strongly they contribute to the log-odds of success. Vectors are simply a compact notation to represent collection of all the covariates in the model and their corresponding parameters, especially when there are hundreds or even thousands of covariates to consider. Usually, it's much easier to grasp the concept of this model in expanded form as given in [this section](#sec-simdata).

The $\pmb \beta$ coefficients are the main parameters of interest (as $\pi_i$ is derived from $\pmb \beta$). Estimates of $\pmb \beta$ are obtained by using a numerical algorithm to maximize the log-likelihood of the model, which is a topic that won't be discussed here to keep the math more approachable.

The assumptions of the model are as follows:

1.  The outcome is a binary random variable, specifically a Bernoulli[^1] random variable.

2.  The independent/explanatory variables (I'll commonly refer to these as covariates) have a linear relationship with the log-odds of "success". The linearity here refers to the beta parameters, for example one may include an $x^2\beta$ term in the model as the beta parameter has a power of 1.

3.  Conditional on the covariates in the data set, the observations are independent of each other.

4.  No multicollinearity between covariates. In layman's terms, for any single covariate, the other covariates in the model don't perfectly describe that covariate.

5.  No complete separation. In other words, the covariates cannot perfectly predict binary outcome.

[^1]: There is an extension of logistic regression to group level data where the outcome is the number of successes observed and is modeled as a binomial random variable.

## Comments on Logistic Regression Compared to Other Machine Learning Models

The important distinction between logistic regression and other machine learning models is that the statistical theory on the model parameters is well understood. For each of the $\beta$ parameters in our model, we know the statistical properties to allow inference through p-values or confidence intervals. Logistic regression is still a tool used today to understand, or *infer*, the relationship between a binary outcome and other data collected when statistical evidence is required to validate any findings.

Although in most real-world scenarios the other machine learning models perform better in terms of predictiveness, logistic regression can sometimes still be a better model in terms of predictiveness. In my experience through classes and work projects, a logistic regression model is actually superior to the more sophisticated machine learning techniques of Gradient Boosted Trees and Random Forests under a few scenarios with respect to the AUC (area under the curve) performance metric.

The first scenario I've come across is when the sample size is relatively small. The machine learning techniques tend to work very well when there is more data to help effectively learn the functional relationship between the data you have and the target outcome. I would say this would be the situation when your sample size is in the magnitude of a few hundreds or less. After that point, other machine learning techniques perform at their expected capacity.

The second scenario where logistic regression can prove better than tree based methods is if one is willing to both select the covariates that strongly relate to the outcome of interest and also identify how each covariate functionally relates to the outcome. To expand on what I mean by "how each covariate functionally relates to the outcome", some examples of functional relationships are the usual linear relationship, quadratic relationship, or sinusoidal relationship. In mathematical terms you can propose the following models:

$Linear: \log(\frac{\pi_i}{1 - \pi_i}) = x_i\beta$

$Quadratic: \log(\frac{\pi_i}{1 - \pi_i}) = x_i\beta_1 + x_i^2\beta_2$

$Sinusoidal: \log(\frac{\pi_i}{1 - \pi_i}) = sin(x_i)\beta$

Choosing a functional relationship alone produces infinite possibilities for one to sort through in addition to just choosing which covariates to include in one's analysis. The strength of tree based approaches is that they learn these relationships with enough data, but of course the tree won't be as accurate as if one were to explicitly identify that "true" relationship when training a logistic regression. However, identifying the "true" functional relationship between the outcome and covariates is rather impractical when one has many different covariates to consider at once. So once again, when the data becomes sufficiently large, it is more practical to use a Gradient Boosted Tree model or Random Forest model.

The advantages and shortfalls of logistic regression compared to other machine learning techniques naturally extend to any generalized linear model such as linear regression. To be clear, I am **NOT** claiming that in these scenarios generalized linear models will **ALWAYS** surpass the newer machine learning models and algorithms, but rather that these scenarios **ALLOW** generalized linear models to shine just as well, if not better, than their other machine learning counterparts in terms of predictiveness.

# A Novel Application Problem {#prob}

Suppose that one is starting an online luxury clothing store selling clothes, shoes, and accessories. They have the production cost of each product and wish to know what markup percentage is optimal for generating the most revenue. The data is set up such that we have many offers for products on sale for different markup percentages and it is recorded whether the sale was made or not. From past data, it is plausible that the type of product (clothes, shoes, accessories) has an affect on whether a sale is made.

Defining and estimating the optimal markup percentage is the novel part of the analysis that I think differentiates this analysis from typical logistic regression analyses where the focus is usually on determining a statistically significant relationship between a covariate and the outcome of interest.

To be clear, this MUCH simplified version of a problem that a friend approached me for consultation. For privacy reasons and to focus more on the methodology used.

# Simulating Toy Data {#sec-simdata}

Before proceeding with the analysis, we shall simulate a toy data set in R that will fit the problem. In the ideal case, I would have found a real world data set that matches our example, but my superficial search of the internet returned only aggregated product sales and wouldn't quite match the situation described above. This does detract from exploratory analyses as I know how the data is generated and greatly simplifies the model building process. However, the general workflow and considerations for building the model would be the same with a real data set.

We will use the following logistic regression model to simulate data:

$$
\log(\frac{\pi_i}{1 - \pi_i}) = \beta_0 + x_{markup} * \beta_{markup} + 
x_{cost} * \beta_{cost} + x_{type=shoes} * \beta_{type=shoes} + x_{type = clothes}*\beta_{type = clothes}
$$

where $x_{markup}$ is the markup percentage above 0 (above 100 is allowed), $x_{cost}$ is cost in dollars, $x_{type=shoes}$ is 1 when product $i$ is shoes and 0 otherwise, and similarly $x_{type = clothes}$ is 1 when product $i$ is clothes and 0 otherwise. When product $i$ is accessories, both $x_{type=shoes}$ and $x_{type = clothes}$ are 0. The way that $x_{type}$ is encoded is known as the dummy variable encoding common in statistics (which is different from the one-hot encoding more prevalent in the machine learning field). The outcome of interest $Y_i$ is a binary variable where 1 indicates the product was sold and 0 the product was not sold, and it follows that $\pi_i$ is the probability that product $i$ is sold or not. To clarify further about the $x_{markup}$ covariate, if $x_{markup} = 20$ then the product was marked up $20\%$.

I chose the values for true parameters $\pmb \beta$ that I felt gave a good spread of the true probabilities while also being representative of the expected behavior in the example problem described to me. I arbitrarily chose a decently large sample size of 3000 to ensure proper estimation.

```{r sim data}
## True parameter values

set.seed(2013)

sample_size <- 3000

true_beta <- c(2, # intercept
               -0.2, # markup
               0.002, # cost
               1, # type=clothes
               1.5 )# type=shoes

### Simulate data

type <- sample(c("accessories", "clothes", "shoes"), 
               size = sample_size,
               replace = TRUE,
               prob = c(0.3, 0.5, 0.2))
cost <- rnorm(sample_size, mean = 2000, sd = 400)
markup <- runif(sample_size, min = 20, max = 120)

## make sure markup and cost are above 0
markup <- ifelse(markup <= 0, 10, markup)
cost <- ifelse(cost <= 0, 2000, cost)

# convert type into a factor variable to prepare for analysis

type <- factor(type, levels = c("accessories", "clothes", "shoes"))

x_df <- data.frame(markup = markup, cost = cost, type = type)

# get x_matrix

x_matrix <- model.matrix(~ 1 + markup + cost + type, data = x_df)

#simulated log-odds per person

log_odds <- x_matrix %*% true_beta

#convert log_odds to probabilities

inverse_logit <- function(x){(1 + exp(-x))^-1}
true_probs <- inverse_logit(log_odds)

# get the outcome
sold <- rbinom(sample_size, 1, true_probs)
x_df$sold <- sold
# x_df$true_probs <- c(true_probs)
# x_df$true_log_odds <- c(log_odds)
```

Here are the first few rows of the data set to get an idea of how the data is organized and its values:

```{r data}
head(x_df, 5) %>%
  kable(align = 'c') %>%
  kable_styling()
```

# Example Logistic Regression Analysis

Before jumping into the analysis or looking at the data, it is imperative to determine whether the analysis is attempting to optimize prediction or inference. Although a model that infers relationships well typically predicts well and vice versa, the criteria statisticians use to judge what model is best favor one over the other. Because of the optimization scheme I outline in a [later section](#sec-optimize), I believe having an unbiased estimate of $\pi_i$ and $\beta_{markup}$ is the main goal, so I interpret this as predominantly an inference problem because of that.

## Quick Exploratory Analysis of the Data Set

When starting an analysis, it's important to first summarize and get a feel of the data. This is a useful and important step as it informs what are reasonable models for the data and indicates the quality of the data. The `gtsummary` package is used to produce the pretty summary tables presented below.

```{r}
#| label: tbl-summ
#| tbl-cap: "Summary Tables"
#| tbl-subcap: 
#|   - "Summary by Sold"
#|   - "Summary by Type"
#| layout-ncol: 2

x_df %>%
  tbl_summary(
    include = c(markup, cost, sold),
    type = list(all_continuous() ~ "continuous2"),
    statistic = list(
      all_continuous() ~ c("{mean} ({sd})", "{median} ({p25}, {p75})", "[{min}, {max}]")
    ),
    by = sold)

x_df %>% 
  tbl_summary(
    include = c(markup, cost, type, sold),
    type = list(all_continuous() ~ "continuous2"),
    statistic = list(
      all_continuous() ~ c("{mean} ({sd})", "{median} ({p25}, {p75})", "[{min}, {max}]")
    ),
    by = type)
```

```{r}
#| label: fig-comp
#| fig-cap: "Comparing cost and markup"

plot(markup, cost, 
     main = "cost vs markup", 
     ylab = "cost of production ($)", 
     xlab = "markup of product (%)")
```

The first table summarizes markup and the cost variables by whether the product sold (the 1 group) or not (the 0 group). Comparing these two groups, we can clearly see that markup is lower in the groups that sold and indicates a negative association between sales and markup. The distribution of cost shifts about 100 dollars higher in the group that sold and thus suggests a positive association between cost and sales.

From the first table, it is important to take note of how many sales (442) and failed sales (2558) are observed in the data set as this guides how many parameters are estimable in the model. A conservative rule-of-thumb is 10 events per parameter in the model where events is defined as the minimum of sales (442) and failed sales (2558). By this rule of thumb, one can reliably have about 44 parameters in the model. To gain intuition about how this is an issue, imagine if there were 0 sales in the data set. Then the only guess one could make based on the data is that every sale will fail (a violation of assumption 5); it is not clear how any of the covariates impact the probability of selling as there is no data on a sale being made.

The second table summarizes the different covariates and outcome by type. We don't notice any difference in the distributions of markup and cost as the quartiles and averages are about the same across the different product types. This implies that type is associated with neither the cost nor markup, and this is a best case scenario for a logistic regression analysis (in relation to assumption 3). The proportion of successful sales is different among the product types with clothes selling at the highest rate followed by shoes and then accessories. This implies that there may be an association between type and product sales.

Clearly, there is an imbalance in the distribution of type in the data set (i.e. the data set is not evenly split to be 1000 for each product type) and that can be potentially problematic for logistic regression estimates. For example, if the shoes product type only had 10 data points, then there would not be enough data to reliably estimate how shoes relate to a successful sale. Luckily in this scenario, we have enough cases (over six hundred) of each product type such that including type in the logistic regression model should be reliably estimable.

A related but much more severe issue is whether there are enough observations of sales or failed sales in each category. In an extreme scenario, imagine whether no shoe sales where made. The intuitive best guess for the model is that if the product type is a shoe then the probability of selling is 0 (which is a violation of assumption 5). Of course, when there are small categories, like in the earlier scenario when shoe sales were only 10, this problem is more likely to occur. The same rule-of-thumb of at least 10 of each sales and failed sales applies here.

In cases where there are covariate imbalance or lack of events in a category, there are a number of options available to ameliorate the problem and each have their pros and cons. Continuing the from the previous example, one option is to simply collect more data on shoe sales. This is the most statistically sound option but might be impractical due to cost or the nature of collecting the data. Another option is to combine categories together that are similar. In the context of this problem, imagine if shoes had only 10 data points. Then it might be advantageous to contextualize the data as whether the sale was accessories or not by combining the shoes and clothes categories together. The problem is justifying whether to combine shoes with the accessories or clothes category. One could use background knowledge about the problem, some measurement of improved model performance, or some sort of clustering algorithm to combine categories together.

Finally, in the scatter plot comparing cost and markup, we observe random spread of the points. There is no indication of a linear or other functional relationship between markup and cost, which is an ideal scenario for logistic regression (again, in relation to assumption 3).

## Building a logistic regression model

Fitting a logistic regression model in R is a simple one line of code. Since we know what the true generating parameters are above, let's double check our work by comparing the fitted values to the true parameters.

```{r}
#| label: tbl-truefit
#| message: false
#| tbl-cap: "Comparing estimates to true values. Estimates are on the log-odds scale"
fit <- glm(sold ~ 1 + markup + cost + type, 
           data = x_df,
           family = binomial)
ci <- suppressMessages(confint(fit))
result <- data.frame(truth = true_beta, 
                     est = coef(fit),
                     ci_lwr = ci[,1],
                     ci_upr = ci[,2])

kable(result, digits = 4, align = 'c') %>% 
  kable_styling()
```

As expected with this rather large sample size, the estimates are fairly close to the true values and are well within the 95% confidence intervals. All of the parameter estimates are statistically significant as 0 is not within the confidence intervals, so this is a convincing model. At this step, one would normally interpret these numerical values as odds ratios. That is not the goal of this analysis, so I will skip the interpretation step.

Regardless of whether interpreting the model coefficients is important for the goal of the analysis, it is important to take a step back and determine whether the coefficients make intuitive sense in terms of the application problem. For continuous covariates, a positive value indicates that as the value of that continuous covariate increases so does the probability of a successful sale. A negative value indicates that as the value of the continuous covariate decreases, the probability of a successful sales decreases.

In terms of continuous covariates in the model at hand, the $\beta_{markup}$ is a negative value so the probability generally decreases as the markup percentage increases. Meanwhile, $\beta_{cost}$ is positive so the probability generally increases as the cost of the good increases. This makes intuitive sense in terms of the problem. As the cost to produce a product increases it is more likely to be of higher quality or made from rarer materials, and that could be enticing for luxury customers. When markup increases, the sales cost of the item increases while the quality of the item doesn't change, so too much markup reduces the bargain from the item.

It is very important to note that the raw numbers given in the table above do not make it immediately obvious how fast or slow the probability of success $\pi_i$ increases or decreases with respect to that continuous covariate without doing a bit of math (see [this section](%7B#sec-probcalc%7D)). The raw numbers are instead related directly to the log-odds of a sale $\log(\frac{\pi_i}{1 - \pi_i})$. In addition to that, at first glance the $\beta_{cost}$ variable seems small relative to the other variables in the model. This is due to cost being in the magnitude of 1000s while markup is in the magnitude of 10s. One could opt to rescale and center covariates in the model so that estimates are on the same magnitude and make the fitting algorithm more stable, but to simplify the model building and interpretation I forego that step.

For categorical variables, one has to consider how they are encoded into the model to interpret the coefficients. In this model, we encoded it such that the intercept is the accessories category and we use it as a baseline to judge the shoes and clothes category. Since both $\beta_{shoes}$ and $\beta_{clothes}$ have positive estimates, the model suggests that both shoes and clothes have a higher probability of selling relative to accessories. Based on the numerical values, shoes is the category with the highest probability of sales, followed by clothes, and finally accessories. This reflects the same trend observed in the observational data analysis, so there doesn't seem to be any peculiar patterns in the model.

### Comparing Different Logistic Regression Models

When comparing two logistic regression models, there are two common statistical performance criteria: the Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC). Both are quantities that expresses how well the proposed model fits the data while also balancing how many parameters are in a model. More parameters in a model results in a better fit to the data but too many parameters results in overfitting. The problem with an overfit model is the lack of generalizability to new data. BIC favors having less parameters in a model as compared to it's counterpart the Akaike Information Criterion (AIC)[^2]. Therefore, the BIC is sometimes the preferred model selection criterion when the goal is to identify what covariates go into a model, and thus BIC aligns with our goal in this analysis. On the other hand, the theory of how AIC is derived indicates that it is superior when the analysis goal is prediction.

[^2]: The penalty term for BIC is $\log(n)n_{par}$, where $n$ is the sample size of the data and $n_{par}$ is the number of parameters, is larger then the penalty term of $2n_{par}$ for AIC. Thus, BIC requires more evidence to add a new parameter into a model than AIC.

One strong advantage of AIC and BIC is that they do not require splitting the data into a training set and test set due to their theoretical derivations. The downside to splitting the data is that using less data to build a model results in less precise estimates. If one were to use an alternative test metric to choose a model, such as prediction accuracy, recall, precision, F1 score, or AUC, then one would need to either use a form of cross-validation or split the data to not have overly optimistic estimates of the data. To elaborate on over optimism, the idea is a statistical model performs best if it is applied to exactly the same data used to build the model. So to estimate these other performance metrics reliably, one needs to apply the model to data not used in building.

One major disadvantage about AIC and BIC is that the formula requires that the model have a log-likelihood and the ability to count the number of parameters in the model. This is mainly a problem for popular modern machine learning models like random forest and neural networks where in the former the typical algorithm doesn't have a log-likelihood and the latter the number of parameters is difficult to count. In these cases, it is better to use the other performance metrics as they do not need log-likelihood or the number of parameters in the model to estimate (this is known as model agnostic performance metrics.)

The actual numerical values of BIC and AIC are only useful when comparing them to the AIC and BIC of other models. To elaborate, a BIC value of 100 doesn't indicate anything about how well the model fits. Instead, lower values of BIC or AIC indicate a better statistical model for the data compared to the BIC or AIC of a different model. A difference of at least 2 is the typical threshold to consider one model superior the other. For differences of less than 2 the models are about equivalent and one must decide which model is superior. In these situations, one may opt for the model with less parameters in favor a simpler model or the context of the application problem can favor the more complicated model.

#### Applying BIC

In addition to the true model, let's consider three other logistic regression models. One without type in the model but all other variables in the model. The hypothesis behind this model is that the product type is not related to the probability of a successful sale.

$$
\log(\frac{\pi_i}{1 - \pi_i}) = \beta_0 + x_{markup} * \beta_{markup} + 
x_{cost} * \beta_{cost}
$$

A second model with a quadratic term for markup and including all the other variables. The hypothesis for this model is that the rate that the log odds increases or decreases depends on the value current value of markup.

$$
\log(\frac{\pi_i}{1 - \pi_i}) = \beta_0 + x_{markup} * \beta_{markup} + x_{markup}^2 * \beta_{markup^2} +
x_{cost} * \beta_{cost} + x_{type=shoes} * \beta_{type=shoes} + x_{type = clothes}*\beta_{type = clothes}
$$

Finally a simple model with just markup. The rationale behind this model is that it is the most convenient form to use in relation to the optimization step outlined in [this section](#sec-optim)

$$
\log(\frac{\pi_i}{1 - \pi_i}) = \beta_0 + x_{markup} * \beta_{markup}
$$

```{r}
#| label: tbl-modselect
#| tbl-cap: "Comparing Logistic Regression Models"
fit1 <- glm(sold ~ 1 + markup + cost, 
           data = x_df,
           family = binomial)
fit2 <- glm(sold ~ 1 + markup + I(markup^2) + cost + type, 
           data = x_df,
           family = binomial)
fit3 <- glm(sold ~ markup, data = x_df, family = binomial)

df_model_selection <- data.frame(model = c("True_model", "no_type", "quadratic_markup", "Simple_markup"), 
                                 bic = c(BIC(fit), BIC(fit1), BIC(fit2), BIC(fit3))
                                 ) 
df_model_selection %>%    
  kable(digits = 2, align = 'c', caption = "Smaller BIC indicates better model") %>%  
  kable_styling()

```

Upon inspecting the BIC values, the true model is unsurprisingly the lowest BIC and thus the best model. Disregarding our knowledge of the true data generating model, the model chosen at this step strongly translates the trends observed in the exploratory data analysis step, and thus highlights the importance of exploring the data before building models. The simple model with just markup is clearly the worst model with the highest BIC, so although the simple model would drastically simplify our optimal estimation of $\pi_i$ in the next step, the data indicates a more complex relationship between the outcome and covariates is more appropriate.

Although we only inspect 4 models here, usually one would explore many more models and spend the bulk of their time trying to figure out the best model. Usually background knowledge of important factors is involved when building the model and some consideration of limitations in the data quality. An example of background knowledge in the context of this problem is that product type seemed important from the sales data of a different luxury store. An example of limitations in the data would be the need to combine shoes and clothes categories together due to a lack of data in one of the categories. More complex relationships like interactions and different functional transformations of continuous variables could also be considered especially if these complex relationships reflect the goal of the analysis.

#### Other Model Building Strategies

In the previous section, we considered models according to some hypotheses about relationships in the data. It is my strong personal preference to use such a strategy as it usually keeps the model building focused on specific questions. However, one might miss out on a logistic regression model that potentially describes the data better if they cannot think of a hypothesis to justify it.

The only method that I know of that is guaranteed to find the best logistic regressions model is to use the best subsets strategy. Best subsets is to consider every single possible combination of covariates in the model and may include interaction terms and nonlinear transformations. With the power of computers nowadays, this is actually a viable strategy to consider, but it will take a long time to fit and evaluate every single possible model. The main concern with employing this strategy is known as multiplicity. Roughly speaking, when the goal is to use p-values or confidence intervals for statistical significance testing, these algorithms test so many models that it is likely to find a significant relationship even when it does not truly exist. This is not so much a problem when the goal is prediction as it is not a concern whether the true relationship between the data and outcome is found.

An alternative strategy is to use [automatic or stepwise selection algorithms](https://en.wikipedia.org/wiki/Stepwise_regression){target="_blank"} such as forward selection or backward selection. The forward selection and backward selection algorithms systematically add or remove a covariate respectively that improves the model the most until the model doesn't improve. These are especially useful when there is absolutely no intuition on how the data relates to the outcome and practically necessary in settings with many covariates to consider. These methods also suffer from the multiplicity problem, and there is no guarantee that these algorithms will arrive at the best subsets model.

Using a penalized regression model that removes parameters as the model is fit in order to choose which parameters to keep in the model avoids the downside of multiplicity while still being useful when there are many covariates to consider. LASSO is the most popular and simplest penalized regression that achieves this. The strategy is to fit the LASSO model and use the covariates selected by the model fit in the logistic regression model. The disadvantage of this strategy is when two covariates are highly related to each other as well as the outcome (a situation known as confounding). To the penalized regression model, the two covariates are more or less equivalent and it'll remove one randomly. This could be a problem if one covariate had more applicable significance than the other but the LASSO model opts to remove it. It is my personal preference to use such a method for model selection.

## Assessing Model Assumptions

Assessing whether the model assumptions are violated is often overlooked when fitting a logistic regression model. The main ones to assess are linearity of covariates (assumption 2) and multicollinearity (assumption 4). The independent sampling assumption (assumption 3) is usually considered when planning the analysis and can potentially investigated through exploratory analysis. Things to look for include covariates that strongly correlate, categorical values that do not have any observed success, and background knowledge about whether independence makes sense. Perfect separation (assumption 5) is normally a warning from R if/when that occurs and possibly identified during exploratory analysis.

### Assessing Multicollinearity

Based on the exploratory analysis earlier, multicollinearity is not likely not an issue in this data set. We know this for a fact because I generated the data independently. The exploratory analysis revealed that there wasn't any clear relationship between pairs of the covariates, and all of the parameters in the model are statistically significant. These are both great signs that multicollinearity is likely not present. However, to formally check for multicollinearity, one can check the Variance Inflation Factors (VIFs) in the covariates. I'll omit checking the VIFs due to the exploratory analysis revealing a high quality data set, but one can read more on VIF theory [here](https://online.stat.psu.edu/stat462/node/180/){target="_blank"} and how to calculate it using the `car` package in R [here](https://www.r-bloggers.com/2023/12/exploring-variance-inflation-factor-vif-in-r-a-practical-guide/){target="_blank"}.

I will note though that if one adds higher order terms in the regression, for example,

$$
\log(\frac{\pi_i}{1 - \pi_i}) = \beta_0 + x_{markup} * \beta_{markup} + x_{markup}^2 * \beta_{markup^2}
$$ then instead of the VIF one will need to inspect the Generalized Variance Inflation Factor (GVIF).

There are a few options to handle multicollinearity when it occurs. The easiest would be to remove covariates from the model that are highly correlated. Although easy to implement, it is arbitrary to decide which covariates to remove. A more sophisticated approach would be to do some variation of a Principal Components Analysis (PCA) to transform the covariate data into one that is perfectly NOT collinear. This improves the precision of the data set by reducing the number of parameters but makes the results difficult to interpret in terms of the original data set. Finally, other models are better suited to approach this issue such as penalized logistic regression models.

### Assessing linearity {#sec-lin}

The hardest assumption to verify and is most often overlooked is the linearity assumption, so I'll go into more detail on how to assess this assumption. This only applies to continuous covariates which are markup and cost in this example. We can use the `binnedplot` function in the `arm` package to assess this. What this function does is bin or discretize the continuous variable into equally spaced segments. Next, calculate the residuals for the observations via:

$$
  residual_i = y_i - \hat \pi_i 
$$

for each data point where $\hat \pi_i$ is calculated from a proposed logistic regression model. Finally, for all the residuals within a bin, calculate the average residual. We inspect the plot to make sure that the residuals are randomly scattered around the $y = 0$ horizontal line, there is no pattern in the residuals, and that at most \~5% of the residuals are outside of the confidence interval lines.

Below is a plot to illustrate how the binned residuals are calculated. The points plotted in a binned residual plot would be the points observed in red.

```{r}
#| label: fig-binbuild
#| fig-cap: "Visual on how binned residual plots are made. The vertical lines represent the beginning/ending a bin and red points the average of all the residuals in a bin. The red points are what is plotted in binned residual plots."

## example on how the binned residual plot is constructed
rng <- range(x_df$cost) # min, max of markup
## break up range into 3 equal parts
breakpoints <- seq(from = rng[1],
                   to = rng[2],
                   by = diff(rng/3))
midpoints <- (breakpoints[-1] + breakpoints[-length(breakpoints)]) / 2
res <- x_df$sold - predict(fit, type = 'response')
res_df <- data.frame(res = res,
           bp = cut(x_df$cost, breaks = breakpoints))
res_ave <- aggregate(res ~ bp,
                     data = res_df,
                     FUN = mean)
plot(x = x_df$cost, y = res, 
     xlab = "cost",
     ylab = "Residuals: observed - predicted",
     main  = "How a binned residual plot is constructed",
     pch = 16)
points(x = midpoints, y = res_ave$res, 
       pch = 16, cex = 1.2,
       col = 'red')
abline(v = breakpoints, col = 'red')
legend("topright", 
       legend = c("observed_residual",
                  "average_residual"),
       col = c('black', 'red'),
       pch = 16)
```

These binned residuals plots assume a large sample size for the statistical properties to hold. Specifically **if the sample size is large within each bin**, then the expected value of these residuals is 0 with known standard errors due to the Lindeberg-Feller central limit theorem. My personal rule-of-thumb is at least 30 observations to be reasonably confident in the approximation, but the more the better. Unfortunately, the binned plots from the `arm` package don't throw warnings when the bins have small sample size, so some caution is needed when interpreting these plots. In addition to requiring large sample sizes, the statistical theory requires values of $\pi_i$ NOT close to 0 or 1. Therefore, any bins with many predicted values of $\pi_i$ close to 0 or 1 are not as informative. To provide intuition on why values of $\pi_i$ close to 0 or 1 are not informative, values of $\pi_i$ equal to exactly 0 or 1 means that all the data was unsuccessful (0) or successful (1), so there is no randomness in the data for the model to build upon in these scenarios.

Let's look at several binned residual plots to assess whether our proposed model is reasonable. The first plot below is binned according the the predicted $\pi_i$ values. This is a nice overall assessment of the model and usually a good starting point of whether assumptions are violated. Most of the binned data points are within the theoretical confidence intervals and fairly close to 0. There isn't a strong pattern that I discern from the residuals, so I am personally satisfied with this plot.

```{r}
#| label: fig-binpreds
#| fig-cap: "Binned residual plot over the expected probabilities. No discernable patterns and points within the confidence interval suggest the linearity assumption is generally met for the model."
res <- residuals(fit, type = "response")
binnedplot(x = fitted(fit),
           y = res, 
           nclass=NULL, 
           xlab="Expected Values", 
           ylab="Average residual", 
           main="Binned residual plot")
```

The next plot below is a binned residual plot where the bins are made according to the markup covariate. The plot displays random scatter within the confidence intervals until the point where markup is 60% or higher. At this point, the binned plot degenerates to 0. This is an expected pattern due to no sales being made for markups above 57%,

```{r}
#| label: fig-binmarkup
#| fig-cap: "Binned residual plot over the expected probabilities. No discernable patterns until markup > 60 where no sales occured. Otherwise, it seems that the linearity assumption is met."
binnedplot(x = x_df$markup,
           y = res, 
           nclass=NULL, 
           xlab="Binned markup", 
           ylab="Average residual", 
           main="Binned residual plot of markup")
```

Below is a binned plot where the bins are defined by the cost variable. This is a nice plot where the random scatter is fairly tight and the values are almost all within the confidence intervals.

```{r}
#| label: fig-bincost
#| fig-cap: "Binned residual plot over the expected probabilities. No discernable patterns and points within the confidence interval suggest the linearity assumption is for the markup covariate."
binnedplot(x = x_df$cost,
           y = res, 
           nclass=NULL, 
           xlab="Binned cost", 
           ylab="Average residual", 
           main="Binned residual plot of cost")
```

Based on the binned plots above, the logistic regression model strongly satisfies the linearity assumption. If there were any patterns in the residual plots, then one would need to refine the logistic regression model with higher order terms (i.e. $x_{cost}^2$) or nonlinear transformations (i.e. $\log(x_{cost})$) to address the patterns. This is not necessary in this project by design, but a future post of mine will explore how to spot different patterns in residual plots and use them to build better logistic regression models.

# Optimizing the Expected Revenue {#sec-optimize}

Now that we have built a satisfactory logistic regression model, we may now proceed to calculating the optimal markup percentage. First we will show how to estimate $\pi_i$ using the logistic regression model and how we can calculate the optimal markup percentage.

## Relationship between $x_{markup}$ and $\pi_i$ {#sec-probcalc}

The relationship between markup and the log-odds might be linear, but when we invert the problem to get the relationship between markup and the probability it is certainly non-linear. To produce this non-linear relationship, we can invert the log-odds back to the probability scale through the following formula:

$$
\pi_i = (1 + \exp[-(\beta_0 + x_{markup} * \beta_{markup} + 
x_{cost} * \beta_{cost} + x_{type=shoes} * \beta_{type=shoes} + x_{type = clothes}*\beta_{type = clothes})])^{-1}
$$

We are specifically interested in how markup relates to the probability of selling, but we have the other data variables in the formula. Before exploring how the probability of selling changes over a range of values for markup, one must first to choose some representative values for cost and type. To start with, let's set the cost level to be the observed average cost level and let type be the most common type in the data set to provide a curve.

```{r}
#| label: fig-probexample
#| fig-cap: "An example of a probability curve with 95% confidence intervals when cost and type are set to the sample average and sample mode respectively"
#| classes: preview-image
## there isn't a mode function in base R
getMode <- function(x){
  lx <- levels(x)
  ux <- unique(lx)
  factor(ux[which.max(tabulate(match(lx, ux)))], levels = lx)
}
range_markup <- seq(from = 0, to = 75, by = 1)
mode_type <- getMode(x_df$type)
mean_cost <- mean(x_df$cost)
new_x_df <- data.frame("markup" = range_markup,
                       "cost" = mean_cost,
                       "type" = mode_type)

preds <- predict(fit, newdata = new_x_df, se.fit = TRUE)
preds_ci_upr <- preds$fit + qnorm(0.975)*preds$se.fit
preds_ci_lwr <- preds$fit - qnorm(0.975)*preds$se.fit
probs_df <- data.frame(pred_prob = inverse_logit(preds$fit),
                       pred_prob_lwr = inverse_logit(preds_ci_lwr),
                       pred_prob_upr = inverse_logit(preds_ci_upr),
                       markup = range_markup)

true_probs <- model.matrix(~ markup + cost + type, data = new_x_df) %*% true_beta
probs_df$true_probs <- inverse_logit(true_probs)

ggplot(probs_df, aes(x = markup)) + 
  ## probability curve predicted from logistic regression in black
  geom_line(aes(y = pred_prob, col = 'Est Prob')) + 
  ## 95% wald CI in red
  geom_ribbon(aes(ymin = pred_prob_lwr, ymax = pred_prob_upr, fill = "95% CI"), alpha = 0.2) + 
  # geom_line(aes(x = markup, y = pred_prob_upr), col = "red") + 
  # geom_line(aes(x = markup, y = pred_prob_lwr), col = "red") + 
  ## probability curve using true values will be in blue
  geom_line(aes(y = true_probs, col = 'True Prob')) + 
  scale_colour_manual(values = c("black", "blue"), name = c("")) +
  scale_fill_manual(values = "#333333", name = "") + 
  ggtitle(paste0("Probablity of Selling Product vs Markup Percentage"),
          subtitle = paste0("type = ", mode_type[1], 
                            "  cost = ", 
                            round(mean_cost[1], digits = 2)
                            )
  ) + 
  labs(x = "markup (%)", 
       y = "Probability of Selling Product") + 
  theme(legend.spacing = unit(0, "pt"),
        legend.margin = margin(t =-16, r = 0, b = -4, l = 0, unit = "pt")
        # legend.key.spacing.y = unit(0, "pt"),
        # legend.key.spacing.x = unit(0, "pt")
        )

```

As expected, due to how well our estimates of the parameters are, the estimated probability curve in black is mostly overlapped by expected probabilities calculated from the true parameters in blue. As we can see from the graph above, the probability of selling decreases as markup increases. This isn't surprising given that the true parameter for markup is -0.02.

Let's plot the differences between the true and predicted probabilities to really emphasize the small the difference in magnitude. We can see that the probabilities only differ at most by 0.02. Depending on the application context, this can be a meaningful difference or not, but for this application let's assume that an underestimation of 0.02 is a reasonably close estimate.

```{r}
#| label: fig-diffprob
#| fig-cap: "Differences between predicted probability and true probability for the generated data. The difference is rather small as expected."
plot(x = probs_df$markup, 
     y = probs_df$true_probs - probs_df$pred_prob,
     type = 'h',
     ylab = "True Prob - Pred Prob",
     xlab = "markup",
     main = "Difference between True and Predicted Probabilities")
```

## Calculating the Optimal Markup

We can propose the following function to express a balance between optimizing cost markup while increasing the probability of successfully selling the product:

$$
f(x_{markup}) = (1 + \frac{x_{markup}}{100})x_{cost} \pi_i
$$

This function is derived from the idea of the expected (aka average) total revenue generated by a new product. The new products total revenue is expressed as $(1 + \frac{x_{markup}}{100})*x_{cost}$ and the expected value of a new observation $Y_i$ is $\pi_i$ because $Y_i$ is a Bernoulli random variable (see [appendix](#sec-exp-return) for a small statistical derivation of this quantity). Thus, the expected total revenue generated from this product is the multiplication of these two quantities.

In the previous section, we calculated how to estimate $\pi_i$ using the logistic regression model and some chosen values for cost and type. So we have all of the needed components to calculate the function above and now we need to determine two things:

1.  Whether a maximum value actually exists

2.  What value of $x_{markup}$ maximizes the function.

The first part is rather easy to show with a graph below. Cost is set to the observed average cost in the data set (`r round(mean(x_df$cost), digits = 2)`) and type is set to the most common type of product in the data set (`r getMode(x_df$type)`).

```{r}
#| label: fig-maxexist
#| fig-cap: "An absolute maximum expected revenue exists at markup = ~20 with revenue= ~2000. Cost was set to the average cost in the data set and type to the mode."
expected_return <- function(markup, cost, type, fit, log = FALSE){
  new_data <- data.frame(markup = markup,
                         cost = cost,
                         type = type)
  
  if(log){
   ### when doing optimization, the log scale is more stable if there are convergence issues
   ### need to restrict to markup > 0
    log(1 + markup/100) + log(cost) + 
      log(predict(fit, newdata = new_data, type = "response"))
  }else{
   (1 + markup/100)*cost * #total sale price
    predict(fit, newdata = new_data, type = "response") #prob success 
  }
}

mode_type <- getMode(x_df$type)
mean_cost <- mean(x_df$cost)
x_markup_vals <- seq(0, 120, by = 0.1)
exp_return_vals <- sapply(x_markup_vals, 
                          expected_return, 
                          cost = mean_cost, 
                          type = mode_type, 
                          fit = fit,
                          log = FALSE)
exp_return_df <- data.frame(x = x_markup_vals,
                            y = exp_return_vals)
ggplot(exp_return_df, aes(x = x, y = y)) + 
  geom_line() + 
  ggtitle(paste0("Expected Revenue vs Markup Percentage"),
          subtitle = paste0("type = ", mode_type[1], 
                            "  cost = ", 
                            round(mean_cost[1], digits = 2)
                            )
  ) + 
  labs(x = "markup (%)", 
       y = "Expected Revenue ($)")
```

From the graph above, it is clear that a maximum occurs around where markup is \~20% and the expected revenue is about \~2100. To calculate the exact value for the optimal markup and expected revenue, we will need to use a numerical optimization algorithm. In R, we can accomplish this using the `optim()` function. There are several different algorithms to choose from, in this case the Broyden–Fletcher–Goldfarb–Shanno (BFGS) quasi-newton method is a great choice. Going into detail of the BFGS algorithm is beyond the scope of this project. The [wikipedia page](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm){target="_blank"} is an excellent starting point for more information.

What is important to note about the numerical optimization algorithms in `optim()` is that they assume that the function is differentiable and continuous with respect to the input variable, in this case $x_{markup}$. This is true for this specific logistic regression model, but can be not true depending on how you encode $x_{markup}$. This is specifically troublesome for the tree-based machine learning models because they discretize continuous variables to build the model. Likewise, any logistic regression model that discretizes $x_{markup}$ would also not work with `optim()`.

Important outputs to inspect from the `optim()` function is `par` which is the markup value where the maximum occurs, `value` which is maximum revenue value, and `convergence` which should be 0 so that you know that the algorithm completed successfully.

```{r optimal calculation}
optimal_vals <- optim(
      # initial guess
      par = 20, 
      # function to optimize
      fn = expected_return, 
      # optimization algorithm
      method = "BFGS",
      # do maximization, default is minimization
      control = list(fnscale = -1),
      # Additional params for function
      cost = mean_cost[1], 
      type = mode_type[1], 
      fit = fit 
      )


if(optimal_vals$convergence != 0){
  stop("The numerical optimization did not converge")
}else{
  new_data <- data.frame(markup = optimal_vals$par,
                         cost = mean_cost[1],
                         type = mode_type[1])
  optimal_prob_success <- inverse_logit(predict(fit, new_data))
  optimal_df <- data.frame(optimal_markup = optimal_vals$par,
                           expected_total_cost = optimal_vals$value,
                           type = mode_type[1],
                           cost = mean_cost[1],
                           prob_success = optimal_prob_success)
  kable(optimal_df, digits = 2, align = 'c') %>%
    kable_styling()
}
```

From this analysis, we find that for an accessory product that costs of \$`r round(optimal_df$cost, digits = 2)` to produce, the optimal markup is about `r round(optimal_df$optimal_markup, digits = 2)` percent. This results in a total revenue generated of \$`r round(optimal_df$expected_total_cost, digits = 2)`.

Although our formula calculates an optimal markup of `r round(optimal_df$optimal_markup, digits = 2)` percent, we need to keep in mind that the range of markup in the data set are values between 20 and 120 percent. Thus, a markup of `r round(optimal_df$optimal_markup, digits = 2)` percent is outside the region covered by the data. I would suggest collecting more data in this range before widely adopting this decision rule.

In this section we only looked at one specific combination of covariates where the product cost is equal to the average cost and the product type is the most common type in the data set. Of course, we may want to consider cases with combinations of product costs and product types. To explore these different scenarios more easily, I've created an [R shiny dashboard](../logisticRegressionShiny/shinyApp.qmd){target="_blank"}.

## Hypothetical Impact of Optimal Markup strategy

Let's assess the business impact of this optimal markup strategy with regards two important business performance metrics:

1.  The change in successful sales compared to the observed sales data

2.  The change in revenue generated compared to the observed revenue

For each point in our data set, we will substitute the observed markup value with the optimal markup value and simulate whether the sale is successful or not. To account for extreme cases, such as when a 99% chance sale fails, I will replicate the experiment 5000 times and calculate the average successful sales and revenue generated and compare these values to the observed revenue and sales.

Calculating the optimal markup for the entire data set is a bit slow, so I used the `future.apply` package in R to run the the calculation in parallel. Going into detail about the `future.apply` package and parallel computing in R is beyond the scope of this project, but I'll make a blog post about it sometime in the future (pun semi-intended).

```{r hypo_rev}
calc_optimal_markup <- function(cost, type, fit){
  optim(par = 20, 
      fn = expected_return, 
      method = "BFGS",
      control = list(fnscale = -1), # do maximization not minimization
      # These params don't change
      cost = cost, type = type, fit = fit 
      )  
}

# non-parallel (sequential) version

# optimal_markups <- mapply(calc_optimal_markup,
#                           cost = x_df$cost,
#                           type = x_df$type,
#                           MoreArgs = list(fit = fit)
#                           )


# set up parallel computing
plan("multisession")
# do the calculation for every data point in parallel
# on my machine this speeds up the calculation about 6 fold
optimal_markups <- future_mapply(calc_optimal_markup, 
                                 cost = x_df$cost, 
                                 type = x_df$type, 
                                 MoreArgs = list(fit = fit)
                                 )
# return computing environment back to normal
plan("sequential")

optimal_x_df <- x_df
optimal_x_df$markup <- unlist(optimal_markups[1,])

## predicted probabilities with optimal markup
hypothetical_probs <- predict(fit, 
                              newdata = optimal_x_df, 
                              type = 'response')

### simulate whether a sale was made for the optimal markup data set, 5000 times
set.seed(2013)
expected_results <- replicate(5000,
          expr = {
            hypothetical_success <- rbinom(sample_size, 1, hypothetical_probs)
            c(change_sales = sum(hypothetical_success) - sum(x_df$sold),
              change_rev = sum(hypothetical_success * (1 + optimal_x_df$markup/100)*x_df$cost) - 
                           sum(x_df$sold * (1 + x_df$markup/100)*x_df$cost)
            )
          })

## create table of observed and optimal results
obs_revenue <- sum(x_df$sold * (1 + x_df$markup/100)*x_df$cost)
optimal_results <- data.frame(observed = c(sum(x_df$sold), obs_revenue),
                              sim_ave = c(floor(mean(expected_results[1,])),
                                          mean(expected_results[2,]))
                              )

optimal_results <- optimal_results %>%
                    mutate(percent_change = 
                             ifelse(sim_ave/observed > 1, 
                                    sim_ave/observed - 1, 
                                    1 - sim_ave/observed),
                           difference = sim_ave - observed
                          )
rownames(optimal_results) <- c("Sales", "Revenue")
## format dollars and percentages as they are hard to read otherwise
optimal_results$percent_change <- scales::percent(optimal_results$percent_change, 
                                                  accuracy = 1, 
                                                  decimal.mark = ".")
optimal_results[2, c(1, 2, 4)] <- 
dollar(unlist(optimal_results[2, c(1, 2, 4)]), 
       accuracy = 0.01, 
       scale = 1/1e6, 
       prefix = "$", 
       suffix = " M")

## print results
optimal_results %>% 
  kable(digits = 2, align = 'c') %>%
  kable_styling() %>%
  footnote(number = "Revenue Rounded to nearest 0.01 Million Dollars")
```

Clearly, the hypothetical impact of using the optimal markup is astounding. Sales increase by nearly 2000 units while revenue increases by \$4.44 million dollars. This translates to 450% increase in sales and 363% increase in revenue, which are astronomical increases! Of course, this is due to the markup values being chosen at random. Any informed strategy in pricing would observe a huge increase relative to this naive strategy.

# Discussion and Conclusion

In this project, we focused on developing a useful logistic regression model and using it to optimize some expected revenue. This method can be extended is several useful directions. First, it is easy to extend the expected revenue formula to take into account profit margins, sales taxes, and costs of storing products (especially if they are perishable products) by adjusting the formula used for optimization. Second, this model and optimization could identify unprofitable products. If the optimal markup is lower than the markup needed to produce a profit, then it would be wise to remove that product from inventory. Finally, one could also use the expected sales to strategize how much stock to keep for certain products.

However, it is important to keep in mind the limitations of a statistical analysis for not only the logistic regression model but any model. Applying estimates from the model assume that the customer base is stable and comparable to the customers observed. For example, external economical factors such as increases in taxes or recessions can change customer behavior not consistent with the estimates of the model. Another consideration would be if one were to grow the customer base in a way that is not consistent with the observed data. If one were to put a discount on shoes for instance, then the consistent customer base may grow to include those specifically interested in shoes and therefore buy shoes at higher rate than the model predicts. Finally, one must also consider that implementing a new pricing strategy inherently changes the behavior of the customers and is not something that this model can consider. An example could be that if the optimal markup is higher than the previous price point customers bought products, then they may wait for a discount as they know the product can be sold for cheaper. Despite the limitations addressed here, collecting and incorporating data from these instances can always help improve these models.

That's it for this project. I hope this was an informative and intriguing read. I'd love to hear your thoughts in the comments below. Mahalo for reading 🤙🏾!

# Appendix 1: Deriving Expected Revenue Formula {#sec-exp-return .appendix}

First, we calculate that $(1 + \frac{x_{markup}}{100})x_{cost}$ would be the sales price of the item (ignoring any taxes).

Next let's derive a simple random variable $Z_i$ to represent the revenue generated from each sale. Consider the random variable $Z_i$ which is a transformation of our previous outcome variable $Y_i$:

$$Z_i = (1 + \frac{x_{markup}}{100})x_{cost} Y_i$$

A straightforward application of expectations shows that

$$\mathbb E(Z_i) = (1 + \frac{x_{markup}}{100})x_{cost} \mathbb E(Y_i) =  (1 + \frac{x_{markup}}{100})x_{cost} \pi_i$$

where $E(Y_i) = \pi_i$ is a known property of the Bernoulli distribution.

From our logistic regression model, we have a (maximum-likelihood) estimate of $\pi_i$ that we will denote as $\hat \pi_i$. Thus our (maximum-likelihood) estimate of $E(Z_i)$ is

$\widehat{\mathbb E(Z_i)} = (1 + \frac{x_{markup}}{100})x_{cost} \hat \pi_i$.

For the more statistically rigorous, the expectations here are conditional on the covariates $\pmb x_i$ in the logistic regression model and $\widehat{\mathbb E(Z_i)}$ is a maximum-likelihood estimator of $\mathbb E(Z_i)$.

# Appendix 2: Deriving Existence of a Maximum Value {.appendix}

I will warn you that this is a math heavy section.

To determine the existence of an maximum value is a straightforward calculus problem. We are specifically looking for a change in the derivative from positive to negative for values of $x_{markup} > 0$. In summary, $\beta_{markup} < 0$ is the only scenario where a maximum exists and is relatively easy to check if an absolute maximum exists.

We need to fully expand the revenue function earlier as

$$
f(x) = c_1(1 + \frac{x}{100})(1 + \exp[-(c_2 + x * \beta )])^{-1}
$$ where I shorten the notation such that $x = x_{markup}$, $c_1 = x_{cost}$, and $c_2$ is the rest of the covariates, i.e. $c_2 = -(\beta_0 + x_{cost} * \beta_{cost} + x_{type=shoes} * \beta_{type=shoes} + x_{type = clothes}*\beta_{type = clothes})$. These values are constants in the function as we are only optimizing markup.

Now we calculate the derivative of this function with respect to $x_{markup}$ to be:

$$
f'(x) = c_1 \left [ \left ( \frac{1}{100} \right ) \frac{1}{1 + \exp[-(c_2 + x \beta )]} + \beta (1 + \frac{x}{100}) \left ( \frac{1}{1 + \exp[-(c_2 + x * \beta )]} \right ) \left ( \frac{\exp[-(c_2 + x * \beta )]}{1 + \exp[-(c_2 + x * \beta )} \right ) \right]
$$

Recall that $\pi_i = \frac{1}{1 + \exp[-(c_2 + x \beta )]}$ and therefore $1 - \pi_i = \frac{\exp[-(c_2 + x \beta )]}{1 + \exp[-(c_2 + x \beta )]}$. Rewriting the above derivative gives the more compact equation:

$$
f'(x) = c_1 \left [ \left ( \frac{1}{100} \right ) \pi_i + \beta (1 + \frac{x}{100}) \pi_i (1 - \pi_i) \right]
$$

**Scenario 1: when** $\beta < 0$

If $\beta < 0$, then $\displaystyle \lim_{x \to \infty} \pi_i = 0$. Applying this to the derivative reveals that

$$ 
\displaystyle \lim_{x \to \infty} f'(x) = 0
$$

Suppose that $f'(0) > 0$, then by Rolle's Theorem there exists some value of $x > 0$ where $f'(x) < 0$. Therefore, we only need to check if $f'(0) > 0$ when $\beta < 0$ to know that a maximum exists.

**Scenario 2: When** $\beta > 0$

If $\beta > 0$, then $\displaystyle \lim_{x \to \infty} \pi_i = 1$ and applying this to the original function now reveals that

$$
\displaystyle \lim_{x \to \infty} f(x) = \infty
$$

Therefore, no absolute maximum exists in this scenario. To interpret this scenario, the customers are more likely to buy the products the more markup percentage there is, so it would be wise to set the markup to infinity.

**Scenario 3: When** $\beta = 0$

For completeness, if $\beta = 0$, then $\pi_i$ is constant with respect to the markup percentage. Therefore the limit for the revenue function is again:

$$
\displaystyle \lim_{x \to \infty} f(x) = \infty
$$

No absolute maximum exists in this scenario. Unlike when $\beta > 0$, customers are indifferent to increases in markup, so one might as well set the markup to infinity.

# Session Info {.appendix}

Below is my R session info that you may refer to in the event the code is not reproducible. I suggest running all chunks in the order presented and only run them once to ensure for the best chance to get reproducible code.

```{r sessInfo}
sessionInfo()
```
